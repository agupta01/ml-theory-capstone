{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00db8ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec3b15-eb00-4843-a273-6b873ac7ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30876a8e-0eb7-4363-a758-2c50f605770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.etl import *\n",
    "from src.rfm import *\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5bf898-52de-451a-878b-f226f4e43d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.gutenberg.org/cache/epub/1533/pg1533-images.html\"\n",
    "\n",
    "make_dataset(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d964ad8-6c43-49fd-9c31-1cb4079b9f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, vocab = tokenizer(\"./data/Author: William Shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f1a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb5f6b9-44b1-49f6-804c-c3fea52c91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = X.reshape(X.shape[0], X.shape[1]*X.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc99c464-0fd7-48b8-9397-3f466bef2d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a01fb30-cc58-4bdd-aed0-f6d291d2db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd63c49",
   "metadata": {},
   "source": [
    "## Baseline: Bigrams/Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b508729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams, trigrams, pad_sequence\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline, flatten\n",
    "from nltk.lm import MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80099fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 32\n",
    "TEST_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9430ad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sent):\n",
    "    # remove newline and separate into characters\n",
    "    sent = list(sent[:-1])\n",
    "    # add start token and truncate to context size\n",
    "    sent = ([\"<START>\"] + sent)[:CONTEXT_SIZE]\n",
    "    # pad to context size\n",
    "    if len(sent) == CONTEXT_SIZE:\n",
    "        return sent\n",
    "    elif len(sent) < CONTEXT_SIZE:\n",
    "        # add end token\n",
    "        sent = sent + [\"<END>\"]\n",
    "        return sent + [\"<PAD>\"] * (CONTEXT_SIZE - len(sent))\n",
    "    else:\n",
    "        raise ValueError(\"Sentence too long after truncating. Something went wrong.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf6238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text\n",
    "\n",
    "fpath = \"./data/Author: William Shakespeare.txt\"\n",
    "raw = open(fpath, \"r\").readlines()\n",
    "sentences = list(map(clean_sentence, raw))\n",
    "train_sentences = sentences[:int(len(sentences) * (1 - TEST_SPLIT))]\n",
    "test_sentences = sentences[int(len(sentences) * (1 - TEST_SPLIT)):]\n",
    "len(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d50e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(train_sentences)[:, :16].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d6e7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shop the dataset in half. The first half will be used to condition the second half\n",
    "CONTEXT_SPLIT_SIZE = 16\n",
    "X_train = np.array(train_sentences)[:, :16]\n",
    "y_train = np.array(train_sentences)[:, 16:]\n",
    "X_test = np.array(test_sentences)[:, :16]\n",
    "y_test = np.array(test_sentences)[:, 16:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23160adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get ngrams\n",
    "ngrams = []\n",
    "for sent in X_train:\n",
    "    ngrams.append(list(bigrams(sent)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45535f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the text\n",
    "vocab = list(flatten(sent for sent in X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10287a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = MLE(3)\n",
    "lm.fit(ngrams, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dae843",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db060ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(lm, X_test, length=16):\n",
    "    # generate text\n",
    "    text = []\n",
    "    for sent in X_test:\n",
    "        text.append(lm.generate(length, text_seed=sent))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818726d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f728b543",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = np.array(generate_text(lm, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b633e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\".join(y_test[0]), \"\".join(y_test_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480bbe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.vocab.unk_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e70f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab():\n",
    "    def __init__(self, vals):\n",
    "        self._dict = {v: i for i, v in enumerate(vals)}\n",
    "        self._dict['<UNK>'] = len(self._dict)\n",
    "        self.rev = {i: v for v, i in self._dict.items()}\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        if key in self._dict:\n",
    "            return self._dict[key]\n",
    "        else:\n",
    "            return self._dict['<UNK>']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._dict)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self._dict)\n",
    "    \n",
    "    def __contains__(self, key):\n",
    "        return key in self._dict\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}({self._dict})\"\n",
    "    \n",
    "    def decode(self, idx):\n",
    "        return self.rev[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f54cc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab = Vocab(list(sorted(lm.vocab.counts.keys())))\n",
    "train_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8f8318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sent, vocab=train_vocab):\n",
    "    return np.array([vocab[w] for w in sent])\n",
    "\n",
    "def evaluate(y_test, y_test_pred):\n",
    "    # evaluate\n",
    "    bleu = []\n",
    "    perplexity = []\n",
    "    y_perplexity = []\n",
    "    for i in range(len(y_test)):\n",
    "        bleu.append(utils.bleu_score([y_test[i]], y_test_pred[i], n=2))\n",
    "        perplexity.append(utils.perplexity(encode(y_test_pred[i])))\n",
    "        y_perplexity.append(utils.perplexity(encode(y_test[i])))\n",
    "    return { \"bleu2\": np.mean(bleu), \"perplexity\": np.mean(perplexity), \"true_perplexity\": np.mean(y_perplexity) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2816e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(lambda x: \"\".join(x), y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14722d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1037e312",
   "metadata": {},
   "source": [
    "## Laplacian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc851f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace each token with its index in the vocab\n",
    "X_train_enc = np.array([encode(sent) for sent in X_train])\n",
    "y_train_enc = np.array([encode(sent) for sent in y_train])\n",
    "X_test_enc = np.array([encode(sent) for sent in X_test])\n",
    "y_test_enc = np.array([encode(sent) for sent in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a18f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the data\n",
    "X_train_enc = np.eye(len(train_vocab))[X_train_enc]\n",
    "X_train_enc = X_train_enc.reshape(X_train_enc.shape[0], X_train_enc.shape[1]*X_train_enc.shape[2])\n",
    "y_train_enc = np.eye(len(train_vocab))[y_train_enc]\n",
    "y_train_enc = y_train_enc.reshape(y_train_enc.shape[0], y_train_enc.shape[1]*y_train_enc.shape[2])\n",
    "X_test_enc = np.eye(len(train_vocab))[X_test_enc]\n",
    "X_test_enc = X_test_enc.reshape(X_test_enc.shape[0], X_test_enc.shape[1]*X_test_enc.shape[2])\n",
    "y_test_enc = np.eye(len(train_vocab))[y_test_enc]\n",
    "y_test_enc = y_test_enc.reshape(y_test_enc.shape[0], y_test_enc.shape[1]*y_test_enc.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2539ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a3b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_char_train = y_train_enc[:, :len(train_vocab)]\n",
    "next_char_test = y_test_enc[:, :len(train_vocab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcb9ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a laplace kernel\n",
    "lam = 1\n",
    "kernel = partial(utils.K_laplace_mat, gamma=0.025)\n",
    "\n",
    "K = kernel(X_train_enc, X_train_enc)\n",
    "alpha_hat = np.linalg.solve(K + lam * np.eye(K.shape[0]), next_char_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0c91a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = kernel(X_test_enc, X_train_enc) @ alpha_hat\n",
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8091f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "(next_char_test.argmax(axis=1) == yhat.argmax(axis=1)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5288a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = np.argmax(yhat, axis=1)\n",
    "yhat[np.arange(yhat.shape[0]), max_index] = 1\n",
    "yhat[yhat != 1] = 0\n",
    "plt.imshow(yhat, aspect=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c968d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot next_char_train, with a good aspect ratio\n",
    "plt.imshow(next_char_test, aspect=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8a7fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_train = kernel(X_train_enc, X_train_enc) @ alpha_hat\n",
    "\n",
    "(next_char_train.argmax(axis=1) == yhat_train.argmax(axis=1)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321951ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_kernel(kernel, alpha_hat, X, z, length=16):\n",
    "    text = []\n",
    "    for i in range(length):\n",
    "        yhat = kernel(X, z) @ alpha_hat\n",
    "        # decode the text\n",
    "        text.append([train_vocab.decode(y) for y in np.argmax(yhat, axis=1)])\n",
    "        # move the window forward\n",
    "        X = np.concatenate([X[:, len(train_vocab):], yhat], axis=1)\n",
    "    \n",
    "    transpose = list(zip(*text))\n",
    "    return transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd50ce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_test = generate_text_kernel(kernel, alpha_hat, X_test_enc, X_train_enc, length=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8a7309",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(lambda x: \"\".join(x), generated_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1733ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(y_test, generated_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff9b35a",
   "metadata": {},
   "source": [
    "## RFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da9152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, M, _ = train_rfm(X_train_enc, next_char_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2ddeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = utils.K_M(X_train_enc, X_train_enc, M, L=1.0) @ alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94baa32f-2983-4618-89b4-3a12d6d3220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.mse(y_train_pred, next_char_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27c487f-cc33-4fe3-914d-f3e84c9e11b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(16, 9))\n",
    "ar = (y_train_pred.shape[1]/y_train_pred.shape[0])\n",
    "ax[0].imshow(utils.softmax(y_train_pred, axis=1), aspect=ar)\n",
    "ax[1].imshow(next_char_train, aspect=ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370623b4-f13a-423a-8251-19f4557c632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = utils.K_M(X_test_enc, X_train_enc, M, L=1.0) @ alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f84c29-5902-4522-97d5-554793e7c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.mse(y_test_pred, next_char_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba95d5c-2df5-455f-afc7-d3675106b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(16, 9))\n",
    "ar = (y_test_pred.shape[1]/y_test_pred.shape[0])\n",
    "y_test_argmax_ohc = np.eye(len(train_vocab))[y_test_pred.argmax(axis=1)]\n",
    "ax[0].imshow(y_test_argmax_ohc, aspect=ar)\n",
    "ax[1].imshow(next_char_test, aspect=ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89bd672",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(y_test_argmax_ohc - next_char_test, aspect=ar)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85480b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7c9568-7c0b-4b04-9917-442c65044393",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_rfm = partial(utils.K_M, M=M, L=1.0)\n",
    "\n",
    "gen_test_rfm = generate_text_kernel(kernel_rfm, alpha, X_test_enc, X_train_enc, length=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62256fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(y_test, gen_test_rfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f71c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(lambda x: \"\".join(x), y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07072a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train and test accuracy\n",
    "train_acc = (next_char_train.argmax(axis=1) == y_train_pred.argmax(axis=1)).mean()\n",
    "test_acc = (next_char_test.argmax(axis=1) == y_test_pred.argmax(axis=1)).mean()\n",
    "\n",
    "print(f\"Train accuracy: {train_acc:.2f}\")\n",
    "print(f\"Test accuracy: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1443a0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "a6a6044cb29be7aa8d18f20f5f922fbbeec34b67744250c81f169bc0b9d02fc2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
